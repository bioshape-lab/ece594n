{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE 594N HW3, Ryan Guajardo, 8619710"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Introduction (Markdown)\n",
    "\n",
    "##### Set the context. What problem is this paper trying to solve? Why is this important?\n",
    "\n",
    "The paper I'm covering is \"Kernel Methods on the Riemannian Manifold of Symmetric Positive Definite Matrices\". They begin the paper by explaining that Symmetric Positive Definite (SPD) matrices are popular for encoding image information and that accounting for the Riemannian manifold of SPD matrices allows many algorithms to be used. They then explain that there is issues with approximating the shape of the manifold locally by using the tangent plane of it, mainly that this is only locally viable and doesnt truly represent the true shape of the manifold. Additionally tangent space mapping is highly computationally intensive because logarithmic and exponential maps are used to iteratively map points from the manifold to the tangent space and back again. They then propose that the solution to this problem lies in mapping SPD matrices to a high dimensional Hilbert space where they can then apply Euclidean geometry. In order to do this they introduce a family of positive definite kernels  on the Reimannian manifold of SPD matrices, which are derived from the Gaussian RBF (Radial Basis Function) but augmented with different notions of distances owed to the Log-Euclidean metric. In the end this is valuable because it allows the use of kernel based algorithms developed for standard Euclidean space to be used, while accounting for the geometry of the Riemannian manifold of SPD matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Brief literature review: Identify 3+ related papers/models and briefly explain how they relate to one another (1 - 2 sentences per paper).\n",
    "\n",
    "\"Fast an Simple Computations on Tensors with Log-Euclidean Metrics\":\n",
    "This paper presents a new family of metrics called Log-Euclidian, the reason this is pertinent to the main paper is because we use the Log-Euclidean metric to represent a true geodesic distance measurement on SPD. We use this true geodesic distance to define an RBF kernel augmented with the Log-Eudclidean distance measurement used throughout the paper. \n",
    "\n",
    "\"Region Covariance: A Fast Descriptor for Detection and Classification\":\n",
    "This paper goes over the method to calculate the region covariance which is used to combine feature vectors in all of the examples shown in the main paper. I had to understand this paper in order to be able to implement the algorithm that I chose to replicate. In this case it was the Texture Classification algorithm. \n",
    "\n",
    "\"Filtering for Texture Classification: A Comparative Study\": This paper introduces the Brodatz dataset and shows ways to extract features from texture images.\n",
    "\n",
    "\"Kernel Analysis over Riemannian Manifolds for Visual Recognition of Actions, Pedestrians and Textures\": In this paper they outline the pitfalls of using tangent spaces to embed Reimannian manifolds in euclidean space. They say tangent spaces only represent \"true\" geodesic distances in certain cases and propose instead to embed the manifold into a Reproducing Kernel Hilbert Space by introducing a Riemannian pseudo kernel. This is pertinent because it is exactly what is proposed in the  main paper I cover, also many of their experiments are very similar. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Background and Model (Markdown)\n",
    "\n",
    "##### Introduce and explain the model and any necessary mathematical background. Use LaTeX for equations.\n",
    "\n",
    "The main idea is that we want to use a kernel to embed the manifold in a RKHS, where we can then apply generalized Euclidean algorithms. According to Mercer's theorem this kernel must be positive definite. The Gaussian kernel is a PD kernel in $R^n$. They propose that replacing the Euclidean distance in the Gaussian Kernel with the geodesic distance on the manifold will do this but it must be positive definite. We are working in the space of $dxd$ SPD matrices, and the metric that is proposed is the log-Euclidean metric, which is shown to be a true representation of the geodesic distance on $Sym_d^+$.\n",
    "\n",
    "##### In general kernel methods can be generalized to non-manifolds like so:\n",
    "Each point x on a non-linear manifold $M$ is mapped to a feature vector $φ(x)$ in a Hilbert space $H$, the Cauchy completion of the space spanned by real-valued functions defined on $M$. A kernel function $k : (M×M) → R$ is used to define the inner product on H, thus making it a Reproducing Kernel Hilbert Space (RKHS). According to Mercer’s theorem, however, only positive definite kernels define valid RKHS.\n",
    "\n",
    "##### Gaussian Radial Basis Function\n",
    "The Gaussian radial basis function (RBF) maps data points to an infinite dimensional Hilbert space. In $R^n$ the Gaussian kernel is $k_G(x_i,x_j):=exp(\\frac{||x_i-x_j||^2}{2\\sigma^2})$. Here the Euclidian distance measure is used. To define this on the Reimannian manifold we use the notion of log-Euclidean Distance defined as $d_g = ||log(X_i)-log(X_j)||_F$ where $||.||_F$ is the frobenius norm induced by the Frobenius inner product. In this way we can accuratley measure the true geodesic distance on $Sym_d^+$ Overall our new kernel can then be written: $k_R(X_i,X_j):=exp(-\\frac{||log(X_i)-log(X_j)||_F^2}{2\\sigma^2})$ and it is also defined as positive definite following Theorem 4.1 in the paper. \n",
    "\n",
    "\"Theorem 4.1. Let (M, d) be a metric space and define $k : (M × M) → R$ by $k(x_i,x_j) := exp(−d^2(x_i,x_j)/2σ2)$. Then, $k$ is a positive definite kernel for all $σ > 0$ if and only if there exists an inner product space $V$ and a function $ψ:M→V$ such that, $d(xi,xj)=∥ψ(xi)−ψ(xj)∥V$. \"\n",
    "\n",
    "\n",
    "##### Kernel PCA on $Sym_d^+$\n",
    "All points $X_i$ are mapped to feature vectors $H$ giving us a transformed set ${\\phi(X_i)}_{i=1}^m$. We then calculate the covariance matrix of the transformed set. This is equivalent to computing the kernel matrix of the original data using $k_R$, the Reimannian Kernel. We then compute the eigenvectors of the kernel matrix. This boils down to a Euclidean representation of the manifold valued data while accounting for the actual geometry of $Sym_d^+$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Implementation (Code) \n",
    "\n",
    "For the implementation portion of the assignment you can look at the Demonstration and Analysis portion where I attempt to replicate the results for 6.3 Texture Recognition using kernel PCA on $Sym_5^+$ with a  Riemannian kernel. The Implementation of their kernel is shown in the section where we define the gram matrix, that section is titled \"define Riemannian kernel for kernel PCA using geomstats\". Here you can see my implementation of their kernel using geomstats elementary operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Demonstration and Analysis (Code / Markdown)\n",
    "\n",
    "Shown below is the implementation of \"6.3 Texture Recognition\" in the paper. The pipeline is.\n",
    "1. Load Brodatz Dataset only 512x512, no Histogram Equalization\n",
    "2. Split each image into 4, 2 for train, 2 for test.\n",
    "3. For every test and train image generate the 5 dimensional feature vectors.\n",
    "4. for each training image, generate 50 random windows 128x128 and compute the covariance descriptors originating from the 5 dim feature vectors.\n",
    "4. for each test image, generate 100 random windows 128x128 and compute the covariance descriptors originating from the 5 dim feature vectors.\n",
    "5. Generate Gram Matrix for using our custom kernel, this is necessary when using a precomputed kernel in Kernel PCA.\n",
    "6. use Kernel PCA on $Sym_5^+$ with the Riemannian Kernel to extract l principle directions and project the training data there\n",
    "6. (part b ) figure out the best sigma value, by cross validation.\n",
    "7. do the same with the test data \n",
    "8. Decide the class of the test data using k=5 nearest neighbor classifier\n",
    "9. Compare to Euclidean Distance Metric\n",
    "\n",
    "Results for this (accuracy and confusion matrix) are shown at the very end in the section titled \"using smallest good sigma value, sigma = 0.4, varying L, Riemannian Metric\" and \"using smallest good sigma value, sigma = 0.4, varying L, Euclidean Metric.\" \n",
    "\n",
    "Here you can see the difference between the Riemannian Metric and the Euclidean Metric similar to what is shown in the paper. The Riemannian Metric performed very well garnering over 90% in all dimensionalities L = [10,11,12,15]. Although it performed well, the amount of computation time was actually quite long, especially when performing the gram matrix computation. I believe in one of the papers outlining the region covariance, \"Region Covariance: A Fast Descriptor for Detection and Classification\", they outline a faster way to do computations, however I did not have time to implement this. \n",
    "\n",
    "In comparison the RBF with Euclidean Metric in place did very poorly, although it was fast. I'm not sure if this is an error? I would assume it is, due to the accuracy measures in the paper being around 90% for all L-dimensional euclidean spaces, but I also understand that implementions will always be different and so I may have not performed the algorithm or pre-processing exactly as they may have. Even performing fitting on the training set it looks like it is struggling to classify points it has already seen, the only thing that seems to help the Euclidean Metric is increasing dimensionality as it gains some ground there and starts to do okay. I tried many fixes and nothing seemed to help aside from increasing the dimesionality. I also tried a standard PCA,(which performed rather well) but that isn't what is outlined in the paper and so it seemed irrelevant. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66utCwija1PK"
   },
   "outputs": [],
   "source": [
    "from skimage.filters import threshold_otsu\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from skimage import color\n",
    "from skimage.filters import threshold_otsu, sobel, gaussian\n",
    "matplotlib.rcParams['figure.figsize'] = (20, 8)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import pandas as pd\n",
    "%pip install opencv-contrib-python\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from google.colab.patches import cv2_imshow\n",
    "! pip install geomstats\n",
    "import geomstats.backend as gs\n",
    "!pip install -U matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lty8PCK8nDqS"
   },
   "source": [
    "# Set Up Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qef7GxkBnJAX"
   },
   "source": [
    "## divide images into 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eVGmqKv_dYBS"
   },
   "outputs": [],
   "source": [
    "def divide_img(img):\n",
    "  x = [0,256]\n",
    "  y = [0,256]\n",
    "  train1 = img[0+x[0]:256+x[0],0+y[0]:256+y[0]]\n",
    "  train2 = img[0+x[0]:256+x[0],0+y[1]:256+y[1]]\n",
    "  test1 = img[0+x[1]:256+x[1],0+y[0]:256+y[0]]\n",
    "  test2 = img[0+x[1]:256+x[1],0+y[1]:256+y[1]]\n",
    "  return train1, train2, test1, test2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKLpcxXqqQby"
   },
   "source": [
    "## Feature Extraction, for Covariance Descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eFsOopii5-IM"
   },
   "outputs": [],
   "source": [
    "def get_feature_space(img):\n",
    "  '''\n",
    "  maps each pixel to a 5 dimensional feature space as in\n",
    "  \"Region Covariance: A Fast Descriptor for Detection and Classification\"\n",
    "  '''\n",
    "  from numpy import linalg as LA\n",
    "\n",
    "  kernely = np.array([[1,1,1],[0,0,0],[-1,-1,-1]])\n",
    "  kernelx = np.array([[1,0,-1],[1,0,-1],[1,0,-1]])\n",
    "\n",
    "  I_x = cv2.filter2D(img,cv2.CV_8U,kernelx)\n",
    "  I_y = cv2.filter2D(img,cv2.CV_8U,kernely)\n",
    "\n",
    "  I_xx = cv2.filter2D(I_x,cv2.CV_8U,kernelx)\n",
    "  I_yy = cv2.filter2D(I_y,cv2.CV_8U,kernely)\n",
    "\n",
    "  F = [img, I_x, I_y, I_xx, I_yy]\n",
    "\n",
    "  return np.array(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SxljxmmnqSA3"
   },
   "outputs": [],
   "source": [
    "def get_features(img):\n",
    "  from numpy import linalg as LA\n",
    "  kernely = np.array([[1,1,1],[0,0,0],[-1,-1,-1]])\n",
    "  kernelx = np.array([[1,0,-1],[1,0,-1],[1,0,-1]])\n",
    "\n",
    "  I_x = cv2.filter2D(img,cv2.CV_8U,kernelx)\n",
    "  I_y = cv2.filter2D(img,cv2.CV_8U,kernely)\n",
    "\n",
    "  I_xx = cv2.filter2D(I_x,cv2.CV_8U,kernelx)\n",
    "  I_yy = cv2.filter2D(I_y,cv2.CV_8U,kernely)\n",
    "\n",
    "  I = np.mean(img)\n",
    "\n",
    "  return [I, LA.norm(I_x,'fro'), LA.norm(I_y,'fro'), LA.norm(I_xx,'fro'), LA.norm(I_yy,'fro')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXKi-LJU8JJS"
   },
   "source": [
    "## get a random window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ShoxiTLk8Kui"
   },
   "outputs": [],
   "source": [
    "def get_random_window(img):\n",
    "  '''\n",
    "  returns a random window with size 128x128 from our input image\n",
    "  '''\n",
    "  import random\n",
    "  window_size = 128\n",
    "  x = random.randrange(0,128,1)\n",
    "  y = random.randrange(0,128,1)\n",
    "  return img[x:x+window_size,y:y+window_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhBSSdf1_Yhg"
   },
   "source": [
    "## Calculate Covariance Descriptors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pL0ZNg-XAKIZ"
   },
   "outputs": [],
   "source": [
    "def get_means(F):\n",
    "  u = []\n",
    "  for i in range(len(F)):\n",
    "    u.append( np.sum(F[i]) / (len(F[i])*len(F[i])) )\n",
    "  return np.array(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "FeCnbIwVHpBE"
   },
   "outputs": [],
   "source": [
    "def zk(x,y,F):\n",
    "  '''\n",
    "  calculate zk for the given index,\n",
    "  zk is a 5 d vector for the given point\n",
    "  '''\n",
    "  zk = []\n",
    "  for i in range(len(F)):\n",
    "    zk.append(F[i][x][y])\n",
    "\n",
    "  return np.array(zk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "MsaneYFu_nfA"
   },
   "outputs": [],
   "source": [
    "def get_covariance_descriptor(F):\n",
    "  '''\n",
    "  get covariance descriptor matrix for particular F calculated with a window\n",
    "  '''\n",
    "  u = get_means(F)\n",
    "  C = np.zeros((5,5))\n",
    "\n",
    "  for i in range(F[0].shape[0]):\n",
    "    for j in range(F[0].shape[0]):\n",
    "      a = zk(i,j,F)-u\n",
    "      C = C+np.dot(a[:,None],a[None,:])\n",
    "\n",
    "  return C/(len(F[0])*len(F[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZTQ3PU-bx0U"
   },
   "source": [
    "# Setup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "yDDPmnbIcHIO"
   },
   "outputs": [],
   "source": [
    "path = '/content/drive/MyDrive/ECE594N/hw3/textures/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDyEFHQRj0VV"
   },
   "source": [
    "## generate paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OvfiXC25cM_r",
    "outputId": "8d4cea27-308a-41cb-b848-a0b284979690"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.1.01.tiff', '1.1.02.tiff', '1.1.03.tiff', '1.1.04.tiff', '1.1.05.tiff', '1.1.06.tiff', '1.1.07.tiff', '1.1.08.tiff', '1.1.09.tiff', '1.1.10.tiff', '1.1.11.tiff', '1.1.12.tiff', '1.1.13.tiff', '1.2.01.tiff', '1.2.02.tiff', '1.2.03.tiff', '1.2.04.tiff', '1.2.05.tiff', '1.2.06.tiff', '1.2.07.tiff', '1.2.08.tiff', '1.2.09.tiff', '1.2.10.tiff', '1.2.11.tiff', '1.2.12.tiff', '1.2.13.tiff']\n"
     ]
    }
   ],
   "source": [
    "# generating paths\n",
    "im_paths = []\n",
    "for j in range(2):\n",
    "  for i in range(13):\n",
    "    if (i+1)<10:\n",
    "      num = '1.'+str(j+1)+'.0'+str(i+1)+'.tiff'\n",
    "      im_paths.append(num)\n",
    "      # print(numbers)\n",
    "    else:\n",
    "      num = str(i+1)\n",
    "      num = '1.'+str(j+1)+'.'+str(i+1)+'.tiff'\n",
    "      im_paths.append(num)\n",
    "      # print(numbers)\n",
    "\n",
    "print(im_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vryjGDIBj1qw"
   },
   "source": [
    "## labels for the textures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Ry4AV7BEeYSh"
   },
   "outputs": [],
   "source": [
    "#labels for each texture in order\n",
    "label = ['Grass',\n",
    "          'Bark',\n",
    "          'Straw',\n",
    "          'Herringbone weave',\n",
    "          'Woolen cloth',\n",
    "          'Pressed calf leather',\n",
    "          'Beach sand',\n",
    "          'Water',\n",
    "          'Wood grain',\n",
    "          'Raffia',\n",
    "          'Pigskin',\n",
    "          'Brick wall',\n",
    "          'Plastic bubbles']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmV23b1Jj4Ba"
   },
   "source": [
    "## put everything into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "RKzgs3FhfFDO"
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "images = []\n",
    "paths = []\n",
    "labels = []\n",
    "\n",
    "for i in range(len(im_paths)):\n",
    "\n",
    "  img = cv2.imread(path+im_paths[i])\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # convert to grayscale\n",
    "\n",
    "  paths.append(path+im_paths[i])\n",
    "  images.append(img)\n",
    "\n",
    "  #restart index for labels after 13\n",
    "  if i+1>13:\n",
    "    labels.append(label[i-13])\n",
    "  else:\n",
    "    labels.append(label[i])\n",
    "  \n",
    "data['paths'] = paths\n",
    "data['images'] = images\n",
    "data['labels'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 865
    },
    "id": "SSnT79bEO27v",
    "outputId": "f28e6651-68f5-4219-ba55-26dd0b28f82f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-2bc132f0-20dd-4020-a3a0-7eb21223e742\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paths</th>\n",
       "      <th>images</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[27, 30, 39, 59, 180, 115, 199, 174, 124, 93,...</td>\n",
       "      <td>Grass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[166, 126, 135, 143, 111, 69, 13, 25, 58, 130...</td>\n",
       "      <td>Bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[164, 188, 201, 186, 170, 157, 141, 151, 133,...</td>\n",
       "      <td>Straw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[219, 192, 203, 170, 166, 133, 150, 165, 162,...</td>\n",
       "      <td>Herringbone weave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[168, 157, 157, 179, 201, 195, 191, 173, 185,...</td>\n",
       "      <td>Woolen cloth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[207, 88, 81, 91, 99, 163, 197, 178, 130, 131...</td>\n",
       "      <td>Pressed calf leather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[161, 174, 167, 156, 191, 202, 194, 153, 137,...</td>\n",
       "      <td>Beach sand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[156, 163, 168, 161, 126, 145, 167, 171, 156,...</td>\n",
       "      <td>Water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[189, 199, 201, 195, 184, 199, 201, 197, 198,...</td>\n",
       "      <td>Wood grain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[150, 182, 188, 187, 184, 180, 185, 195, 168,...</td>\n",
       "      <td>Raffia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[177, 160, 152, 144, 138, 180, 189, 185, 184,...</td>\n",
       "      <td>Pigskin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[186, 210, 215, 209, 198, 185, 126, 106, 87, ...</td>\n",
       "      <td>Brick wall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[180, 118, 126, 126, 81, 110, 115, 115, 114, ...</td>\n",
       "      <td>Plastic bubbles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[7, 9, 16, 33, 208, 97, 236, 198, 110, 68, 42...</td>\n",
       "      <td>Grass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[141, 67, 79, 92, 51, 21, 0, 3, 16, 72, 55, 4...</td>\n",
       "      <td>Bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[129, 195, 229, 189, 144, 112, 77, 98, 64, 72...</td>\n",
       "      <td>Straw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[255, 174, 213, 107, 95, 25, 53, 92, 83, 95, ...</td>\n",
       "      <td>Herringbone weave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[205, 170, 170, 230, 252, 248, 245, 218, 239,...</td>\n",
       "      <td>Woolen cloth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[253, 49, 26, 57, 76, 196, 245, 220, 136, 138...</td>\n",
       "      <td>Pressed calf leather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[130, 189, 157, 108, 245, 253, 248, 97, 53, 1...</td>\n",
       "      <td>Beach sand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[84, 138, 175, 123, 1, 25, 168, 194, 84, 7, 1...</td>\n",
       "      <td>Water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[142, 207, 220, 181, 116, 207, 220, 194, 200,...</td>\n",
       "      <td>Wood grain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[100, 193, 209, 206, 198, 187, 201, 227, 153,...</td>\n",
       "      <td>Raffia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[172, 97, 67, 43, 30, 184, 219, 205, 201, 180...</td>\n",
       "      <td>Pigskin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[220, 251, 253, 251, 242, 217, 31, 20, 11, 5,...</td>\n",
       "      <td>Brick wall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[244, 148, 165, 165, 62, 128, 140, 140, 138, ...</td>\n",
       "      <td>Plastic bubbles</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2bc132f0-20dd-4020-a3a0-7eb21223e742')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-2bc132f0-20dd-4020-a3a0-7eb21223e742 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-2bc132f0-20dd-4020-a3a0-7eb21223e742');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                                paths  \\\n",
       "0   /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "1   /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "2   /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "3   /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "4   /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "5   /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "6   /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "7   /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "8   /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "9   /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "10  /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "11  /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "12  /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "13  /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "14  /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "15  /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "16  /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "17  /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "18  /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "19  /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "20  /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "21  /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "22  /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "23  /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "24  /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "25  /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "\n",
       "                                               images                labels  \n",
       "0   [[27, 30, 39, 59, 180, 115, 199, 174, 124, 93,...                 Grass  \n",
       "1   [[166, 126, 135, 143, 111, 69, 13, 25, 58, 130...                  Bark  \n",
       "2   [[164, 188, 201, 186, 170, 157, 141, 151, 133,...                 Straw  \n",
       "3   [[219, 192, 203, 170, 166, 133, 150, 165, 162,...     Herringbone weave  \n",
       "4   [[168, 157, 157, 179, 201, 195, 191, 173, 185,...          Woolen cloth  \n",
       "5   [[207, 88, 81, 91, 99, 163, 197, 178, 130, 131...  Pressed calf leather  \n",
       "6   [[161, 174, 167, 156, 191, 202, 194, 153, 137,...            Beach sand  \n",
       "7   [[156, 163, 168, 161, 126, 145, 167, 171, 156,...                 Water  \n",
       "8   [[189, 199, 201, 195, 184, 199, 201, 197, 198,...            Wood grain  \n",
       "9   [[150, 182, 188, 187, 184, 180, 185, 195, 168,...                Raffia  \n",
       "10  [[177, 160, 152, 144, 138, 180, 189, 185, 184,...               Pigskin  \n",
       "11  [[186, 210, 215, 209, 198, 185, 126, 106, 87, ...            Brick wall  \n",
       "12  [[180, 118, 126, 126, 81, 110, 115, 115, 114, ...       Plastic bubbles  \n",
       "13  [[7, 9, 16, 33, 208, 97, 236, 198, 110, 68, 42...                 Grass  \n",
       "14  [[141, 67, 79, 92, 51, 21, 0, 3, 16, 72, 55, 4...                  Bark  \n",
       "15  [[129, 195, 229, 189, 144, 112, 77, 98, 64, 72...                 Straw  \n",
       "16  [[255, 174, 213, 107, 95, 25, 53, 92, 83, 95, ...     Herringbone weave  \n",
       "17  [[205, 170, 170, 230, 252, 248, 245, 218, 239,...          Woolen cloth  \n",
       "18  [[253, 49, 26, 57, 76, 196, 245, 220, 136, 138...  Pressed calf leather  \n",
       "19  [[130, 189, 157, 108, 245, 253, 248, 97, 53, 1...            Beach sand  \n",
       "20  [[84, 138, 175, 123, 1, 25, 168, 194, 84, 7, 1...                 Water  \n",
       "21  [[142, 207, 220, 181, 116, 207, 220, 194, 200,...            Wood grain  \n",
       "22  [[100, 193, 209, 206, 198, 187, 201, 227, 153,...                Raffia  \n",
       "23  [[172, 97, 67, 43, 30, 184, 219, 205, 201, 180...               Pigskin  \n",
       "24  [[220, 251, 253, 251, 242, 217, 31, 20, 11, 5,...            Brick wall  \n",
       "25  [[244, 148, 165, 165, 62, 128, 140, 140, 138, ...       Plastic bubbles  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3J095AnIPY-d"
   },
   "source": [
    "## augment dataframe for no HE images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "E42225JxjYi-"
   },
   "outputs": [],
   "source": [
    "less_data = data.drop(data.index[range(13,26)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "id": "IHZLg0_3PCAs",
    "outputId": "f17b4135-6ed5-4eca-cb9a-d43199e70079"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-370ade15-41fe-4d5c-9b53-7baab13cd6ae\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paths</th>\n",
       "      <th>images</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[27, 30, 39, 59, 180, 115, 199, 174, 124, 93,...</td>\n",
       "      <td>Grass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[166, 126, 135, 143, 111, 69, 13, 25, 58, 130...</td>\n",
       "      <td>Bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[164, 188, 201, 186, 170, 157, 141, 151, 133,...</td>\n",
       "      <td>Straw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[219, 192, 203, 170, 166, 133, 150, 165, 162,...</td>\n",
       "      <td>Herringbone weave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[168, 157, 157, 179, 201, 195, 191, 173, 185,...</td>\n",
       "      <td>Woolen cloth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[207, 88, 81, 91, 99, 163, 197, 178, 130, 131...</td>\n",
       "      <td>Pressed calf leather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[161, 174, 167, 156, 191, 202, 194, 153, 137,...</td>\n",
       "      <td>Beach sand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[156, 163, 168, 161, 126, 145, 167, 171, 156,...</td>\n",
       "      <td>Water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[189, 199, 201, 195, 184, 199, 201, 197, 198,...</td>\n",
       "      <td>Wood grain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[150, 182, 188, 187, 184, 180, 185, 195, 168,...</td>\n",
       "      <td>Raffia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[177, 160, 152, 144, 138, 180, 189, 185, 184,...</td>\n",
       "      <td>Pigskin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[186, 210, 215, 209, 198, 185, 126, 106, 87, ...</td>\n",
       "      <td>Brick wall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>/content/drive/MyDrive/ECE594N/hw3/textures/1....</td>\n",
       "      <td>[[180, 118, 126, 126, 81, 110, 115, 115, 114, ...</td>\n",
       "      <td>Plastic bubbles</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-370ade15-41fe-4d5c-9b53-7baab13cd6ae')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-370ade15-41fe-4d5c-9b53-7baab13cd6ae button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-370ade15-41fe-4d5c-9b53-7baab13cd6ae');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                                paths  \\\n",
       "0   /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "1   /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "2   /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "3   /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "4   /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "5   /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "6   /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "7   /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "8   /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "9   /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "10  /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "11  /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "12  /content/drive/MyDrive/ECE594N/hw3/textures/1....   \n",
       "\n",
       "                                               images                labels  \n",
       "0   [[27, 30, 39, 59, 180, 115, 199, 174, 124, 93,...                 Grass  \n",
       "1   [[166, 126, 135, 143, 111, 69, 13, 25, 58, 130...                  Bark  \n",
       "2   [[164, 188, 201, 186, 170, 157, 141, 151, 133,...                 Straw  \n",
       "3   [[219, 192, 203, 170, 166, 133, 150, 165, 162,...     Herringbone weave  \n",
       "4   [[168, 157, 157, 179, 201, 195, 191, 173, 185,...          Woolen cloth  \n",
       "5   [[207, 88, 81, 91, 99, 163, 197, 178, 130, 131...  Pressed calf leather  \n",
       "6   [[161, 174, 167, 156, 191, 202, 194, 153, 137,...            Beach sand  \n",
       "7   [[156, 163, 168, 161, 126, 145, 167, 171, 156,...                 Water  \n",
       "8   [[189, 199, 201, 195, 184, 199, 201, 197, 198,...            Wood grain  \n",
       "9   [[150, 182, 188, 187, 184, 180, 185, 195, 168,...                Raffia  \n",
       "10  [[177, 160, 152, 144, 138, 180, 189, 185, 184,...               Pigskin  \n",
       "11  [[186, 210, 215, 209, 198, 185, 126, 106, 87, ...            Brick wall  \n",
       "12  [[180, 118, 126, 126, 81, 110, 115, 115, 114, ...       Plastic bubbles  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "less_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EyXZlbf8kHwR"
   },
   "source": [
    "# Make train and test dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "z69ABk-8Pzx-"
   },
   "outputs": [],
   "source": [
    "def generate_train_test(data):\n",
    "  trainData= pd.DataFrame()\n",
    "  testData = pd.DataFrame() \n",
    "\n",
    "  testImages = []\n",
    "\n",
    "  trainImages = []\n",
    "  trainLabels = []\n",
    "\n",
    "  for i in range(len(data)):\n",
    "    img = data['images'][i]\n",
    "\n",
    "    train1,train2,test1,test2 = divide_img(img)\n",
    "    trainImages.append(train1)\n",
    "    trainImages.append(train2)\n",
    "    testImages.append(test1)\n",
    "    testImages.append(test2)\n",
    "\n",
    "    trainLabels.append(data['labels'][i])\n",
    "    trainLabels.append(data['labels'][i])\n",
    "\n",
    "  trainData['images']=trainImages\n",
    "  trainData['labels']=trainLabels\n",
    "  testData['images']=testImages\n",
    "  return trainData, testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ue7_3G3nQDMf"
   },
   "outputs": [],
   "source": [
    "less_trainData, less_testData = generate_train_test(less_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnvwDE4zoZnj"
   },
   "source": [
    "# generate covariance descriptors for train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "EEmcyCYSRdCM"
   },
   "outputs": [],
   "source": [
    "def generate_train_test_c(trainData, testData):\n",
    "  # 50 128x128 window covariance descriptors for trainDataC\n",
    "  # 100 128x128 window covariance descriptors for testDataC\n",
    "  trainDataC = pd.DataFrame()\n",
    "  testDataC = pd.DataFrame()\n",
    "\n",
    "  # 50 128x128 window covariance descriptors for trainDataC\n",
    "  trainC=[]\n",
    "  trainLabelsC = []\n",
    "\n",
    "  for j in range(len(trainData)):\n",
    "    image = trainData['images'][j]\n",
    "    print(j)\n",
    "\n",
    "    for i in range(50):\n",
    "      window = get_random_window(image)\n",
    "      F=get_feature_space(window)\n",
    "      C = get_covariance_descriptor(F)\n",
    "      trainC.append(C)\n",
    "      trainLabelsC.append(trainData['labels'][j])\n",
    "\n",
    "  trainDataC['Covariance Descriptor'] = trainC\n",
    "  trainDataC['labels'] = trainLabelsC\n",
    "\n",
    "  # 100 128x128 window covariance descriptors for testDataC\n",
    "  testC=[]\n",
    "  testLabelsC = []\n",
    "\n",
    "  for j in range(len(trainData)):\n",
    "    image = testData['images'][j]\n",
    "    print(j)\n",
    "\n",
    "    for i in range(100):\n",
    "      window = get_random_window(image)\n",
    "      F=get_feature_space(window)\n",
    "      C = get_covariance_descriptor(F)\n",
    "      testC.append(C)\n",
    "      testLabelsC.append(trainData['labels'][j])\n",
    "\n",
    "  testDataC['Covariance Descriptor'] = testC\n",
    "  testDataC['labels'] = testLabelsC\n",
    "\n",
    "  return trainDataC, testDataC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ce5PN6K5R243"
   },
   "outputs": [],
   "source": [
    "less_trainDataC, less_testDataC = generate_train_test_c(less_trainData,less_testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIHXWKAuxltU"
   },
   "source": [
    "# Save Dataframes, since computing these covariances takes forever..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "yic04T3qU0E9"
   },
   "outputs": [],
   "source": [
    "#less data, no HE images\n",
    "less_data.to_pickle('/content/drive/MyDrive/ECE594N/hw3/'+'less_data.pkl')\n",
    "\n",
    "less_testData.to_pickle('/content/drive/MyDrive/ECE594N/hw3/'+'less_testData.pkl')\n",
    "less_trainData.to_pickle('/content/drive/MyDrive/ECE594N/hw3/'+'less_trainData.pkl')\n",
    "\n",
    "less_testDataC.to_pickle('/content/drive/MyDrive/ECE594N/hw3/'+'less_testDataC.pkl')\n",
    "less_trainDataC.to_pickle('/content/drive/MyDrive/ECE594N/hw3/'+'less_trainDataC.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "a86Oc7xOxALb"
   },
   "outputs": [],
   "source": [
    "#full data, with HE images\n",
    "data.to_pickle('/content/drive/MyDrive/ECE594N/hw3/'+'data.pkl')\n",
    "\n",
    "testData.to_pickle('/content/drive/MyDrive/ECE594N/hw3/'+'testData.pkl')\n",
    "trainData.to_pickle('/content/drive/MyDrive/ECE594N/hw3/'+'trainData.pkl')\n",
    "\n",
    "testDataC.to_pickle('/content/drive/MyDrive/ECE594N/hw3/'+'testDataC.pkl')\n",
    "trainDataC.to_pickle('/content/drive/MyDrive/ECE594N/hw3/'+'trainDataC.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DA9S6eWk39L_"
   },
   "source": [
    "# Load data back into project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "7EzAFotkVBIq"
   },
   "outputs": [],
   "source": [
    "#less data, no HE images\n",
    "less_data = pd.read_pickle('/content/drive/MyDrive/ECE594N/hw3/'+'less_data.pkl')\n",
    "less_testData = pd.read_pickle('/content/drive/MyDrive/ECE594N/hw3/'+'less_testData.pkl')\n",
    "less_trainData = pd.read_pickle('/content/drive/MyDrive/ECE594N/hw3/'+'less_trainData.pkl')\n",
    "\n",
    "less_testDataC = pd.read_pickle('/content/drive/MyDrive/ECE594N/hw3/'+'less_testDataC.pkl')\n",
    "less_trainDataC = pd.read_pickle('/content/drive/MyDrive/ECE594N/hw3/'+'less_trainDataC.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "L2JMANJj4JZw"
   },
   "outputs": [],
   "source": [
    "#full data, with HE images\n",
    "data = pd.read_pickle('/content/drive/MyDrive/ECE594N/hw3/'+'data.pkl')\n",
    "testData = pd.read_pickle('/content/drive/MyDrive/ECE594N/hw3/'+'testData.pkl')\n",
    "trainData = pd.read_pickle('/content/drive/MyDrive/ECE594N/hw3/'+'trainData.pkl')\n",
    "\n",
    "testDataC = pd.read_pickle('/content/drive/MyDrive/ECE594N/hw3/'+'testDataC.pkl')\n",
    "trainDataC = pd.read_pickle('/content/drive/MyDrive/ECE594N/hw3/'+'trainDataC.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yy7fBzGx2-Ah"
   },
   "source": [
    "# define Riemannian kernel for kernel PCA using geomstats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "jkE-P1iNBF8c"
   },
   "outputs": [],
   "source": [
    "from geomstats.geometry.spd_matrices import SPDMatrices\n",
    "from geomstats.geometry.matrices import Matrices, MatricesMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "XVbpVuaF8Ydn"
   },
   "outputs": [],
   "source": [
    "def dist( point_a, point_b):\n",
    "  \"\"\"Compute log euclidean distance.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  point_a : array-like, shape=[..., dim]\n",
    "      Point.\n",
    "  point_b : array-like, shape=[..., dim]\n",
    "      Point.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  dist : array-like, shape=[...,]\n",
    "      Distance.\n",
    "  \"\"\"\n",
    "\n",
    "  from numpy import linalg as LA\n",
    "  log_a = SPDMatrices.logm(point_a)\n",
    "  log_b = SPDMatrices.logm(point_b)\n",
    "  A = log_a-log_b\n",
    "  return LA.norm(A,'fro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "kop91DAW29iF"
   },
   "outputs": [],
   "source": [
    "def my_kernel(X, Y,sigma = 0.1):\n",
    "    \"\"\"\n",
    "    My Reimannian kernel:\n",
    "    \"\"\"\n",
    "    from numpy import linalg as LA\n",
    "    kr = gs.exp( (-(dist(X,Y)**2)) / (2*(sigma**2)) )\n",
    "    return kr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BED1liCB-Jo"
   },
   "source": [
    "# Use kernel PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_MeIR2ubQDy"
   },
   "source": [
    "## calculate gram matrix for training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "hhpi9b4DVkl7"
   },
   "outputs": [],
   "source": [
    "def generate_gram_matrix(X,Y,sigma = 0.1, kernel = my_kernel):\n",
    "  '''\n",
    "  generates the gram matrix for using a custom kernel with kernel pca\n",
    "  X and Y are dataframes with 'Covariance Descriptor' columns\n",
    "  '''\n",
    "  #precomputing the gram matrix\n",
    "  G = []\n",
    "  for j in range(len(X['Covariance Descriptor'])):\n",
    "    for i in range(len(Y['Covariance Descriptor'])):\n",
    "      G.append(my_kernel(X['Covariance Descriptor'][j],Y['Covariance Descriptor'][i], sigma))\n",
    "\n",
    "    # print(j)\n",
    "    \n",
    "  G = np.array(G)\n",
    "  G = np.reshape(G,(X.shape[0],Y.shape[0]))\n",
    "  return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88Lrg1_9XQEp"
   },
   "outputs": [],
   "source": [
    "less_G = generate_gram_matrix(less_trainDataC,less_trainDataC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "EtjDgQ9mZrlj"
   },
   "outputs": [],
   "source": [
    "np.save('/content/drive/MyDrive/ECE594N/hw3/'+'less_G.npy',less_G)\n",
    "less_G = np.load('/content/drive/MyDrive/ECE594N/hw3/'+'less_G.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KRySrqcblyh"
   },
   "source": [
    "### save/load gram matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rl-WrHvrc4Q3"
   },
   "outputs": [],
   "source": [
    "np.save('/content/drive/MyDrive/ECE594N/hw3/'+'G.npy',G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_Bslvu4bkGE"
   },
   "outputs": [],
   "source": [
    "G = np.load('/content/drive/MyDrive/ECE594N/hw3/'+'G.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnmWbKPSb30d"
   },
   "source": [
    "## Use kernel PCA with precomputed gram matrix for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PexQuJe0b7F0",
    "outputId": "4e656b6f-701b-4d4a-9f7f-42d2328f9b4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2600, 10)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "transformer = KernelPCA(n_components=10, kernel='precomputed')\n",
    "X_transformed = transformer.fit_transform(G)\n",
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XC4_F5PkcVIR",
    "outputId": "227986b3-37af-4d8e-889a-ab3ac4ccb9d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03417568,  0.00945475, -0.08399346, ..., -0.32723283,\n",
       "        -0.55862632,  0.41073195],\n",
       "       [-0.03533698,  0.00978737, -0.08696432, ..., -0.33685276,\n",
       "        -0.57350083,  0.41983963],\n",
       "       [-0.03038842,  0.00829328, -0.07325832, ..., -0.2754946 ,\n",
       "        -0.46795075,  0.34215576],\n",
       "       ...,\n",
       "       [-0.02041733,  0.00484138, -0.03959354, ..., -0.02871516,\n",
       "         0.00257827, -0.05597037],\n",
       "       [-0.02015779,  0.00477368, -0.03901905, ..., -0.02820234,\n",
       "         0.00253089, -0.05490564],\n",
       "       [-0.01878257,  0.00440835, -0.03589424, ..., -0.02524323,\n",
       "         0.00225444, -0.04860936]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0uc2DFpBCcxC"
   },
   "outputs": [],
   "source": [
    "np.save('/content/drive/MyDrive/ECE594N/hw3/'+'X_transformed.npy',X_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZeBzgeobei5U"
   },
   "source": [
    "## calculate gram matrix for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HvGduNXueupf"
   },
   "outputs": [],
   "source": [
    "testDataC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rFMyWU98ervs"
   },
   "outputs": [],
   "source": [
    "#precomputing the gram matrix\n",
    "Gtest = []\n",
    "for j in range(len(testDataC['Covariance Descriptor'])):\n",
    "  for i in range(len(trainDataC['Covariance Descriptor'])):\n",
    "    Gtest.append(my_kernel(testDataC['Covariance Descriptor'][j],trainDataC['Covariance Descriptor'][i]))\n",
    "\n",
    "  print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_bnPbHu4Pb-"
   },
   "outputs": [],
   "source": [
    "Gtest = np.array(Gtest)\n",
    "Gtest = np.reshape(Gtest,(5200,2600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7-mfkfQB4Vpi",
    "outputId": "75a5f718-8d6e-4b75-ab4b-c0dc04fdcb9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5200, 2600)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-FZaRw6w4JxK"
   },
   "outputs": [],
   "source": [
    "np.save('/content/drive/MyDrive/ECE594N/hw3/'+'Gtest.npy',Gtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m8hH2AVL4YJu"
   },
   "outputs": [],
   "source": [
    "Gtest = np.load('/content/drive/MyDrive/ECE594N/hw3/'+'Gtest.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQjIvJYqCXk7"
   },
   "source": [
    "## use kernel pca for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Rcl_tQ2DSyW"
   },
   "outputs": [],
   "source": [
    "transformer = KernelPCA(n_components=10, kernel='precomputed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PmhYmz1tCUzQ",
    "outputId": "dd9677ec-d5ee-4fbc-8b55-e5c992466283"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5200, 10)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest_transformed = transformer.transform(Gtest)\n",
    "Xtest_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Ey3dLznDuFp",
    "outputId": "86c3a8b5-5e87-40d0-a328-0cc3894f6f66"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01303235,  0.0029131 , -0.02326809, ..., -0.02147553,\n",
       "        -0.01682494, -0.00557485],\n",
       "       [-0.01262491,  0.00279137, -0.02215628, ..., -0.01662819,\n",
       "        -0.00851234, -0.01167648],\n",
       "       [-0.01308604,  0.00293029, -0.02342984, ..., -0.02235992,\n",
       "        -0.01840967, -0.0043423 ],\n",
       "       ...,\n",
       "       [-0.01711853,  0.00396499, -0.0320968 , ..., -0.02160531,\n",
       "         0.00191351, -0.04081956],\n",
       "       [-0.01479652,  0.00333174, -0.02661586, ..., -0.01589901,\n",
       "         0.00137004, -0.02812595],\n",
       "       [-0.01552616,  0.0035198 , -0.02820271, ..., -0.0172423 ,\n",
       "         0.00149322, -0.03084748]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XDgEGWUETGf"
   },
   "source": [
    "# Implement knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N78mtM3iJAVX"
   },
   "source": [
    "## checking data on itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76qwYU_yI_Yu",
    "outputId": "e3e37ea1-041f-48a3-c5b9-d0cd5ba683f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7707692307692308\n",
      "[[161   0   6   0   0   0  10   0   6   1   6  10   0]\n",
      " [  0 196   0   0   0   0   0   0   0   0   0   4   0]\n",
      " [ 20   0  84   0   0  14   9   0   0  46  22   5   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  2   0   0   0 169   0   0   0   2   0   0  27   0]\n",
      " [  3   0  21   0   0 161   4   0   0   0   9   2   0]\n",
      " [ 22   0  12   0   0   9 137   0   4   6   7   3   0]\n",
      " [  0   0   0   0   0   0   0 200   0   0   0   0   0]\n",
      " [  0   0   0   0   0   1  23   0 175   0   1   0   0]\n",
      " [  0   0  31   0   0   0   0   0   0 141  27   1   0]\n",
      " [  6   0  26   0   0  17   3   0   0  39 105   4   0]\n",
      " [ 25   0  11   0  36   0   8   0   3  29  13  75   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0 200]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#convert dataframe labels to list of targets\n",
    "ytrain = trainDataC['labels'].values.tolist()\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=10)\n",
    "neigh.fit(X_transformed, ytrain)\n",
    "\n",
    "yhat = neigh.predict(X_transformed)\n",
    "\n",
    "print(accuracy_score(ytrain,yhat))\n",
    "cm = confusion_matrix(ytrain, yhat)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQUGEpLQF9Gt"
   },
   "source": [
    "## predict labels for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "10secCy_uAfv",
    "outputId": "d9811c1a-08c8-4b29-cccc-fb81b5a5f01a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3151923076923077\n",
      "[[ 90   0  68   0   7   0  48   0  91  34  20  42   0]\n",
      " [  0 393   0   0   0   0   0   0   0   6   0   1   0]\n",
      " [  0   0   2   0   0   0   0   0   0 398   0   0   0]\n",
      " [  0  83   0   3   0   0   0   0   0 314   0   0   0]\n",
      " [  1   6  12   0 279   0   6   0  44  11   0  41   0]\n",
      " [ 29   0  53   0   0 150  36   0   0 105   3  24   0]\n",
      " [ 50   0 142   0   0  10  74   0   0  55  10  59   0]\n",
      " [  0   0   0  33   0   0   0 197   0 170   0   0   0]\n",
      " [  3   0  52   0   0   0  11   0  14 306   1  13   0]\n",
      " [  0   0  17   0   0   0   0   0   0 382   1   0   0]\n",
      " [  2   0 111   0   0   2  35   0   0 206  25  19   0]\n",
      " [  5   0  78   0  13   0  10   0   6 255   8  25   0]\n",
      " [  0   0  12   0   0   0   0   0   0 383   0   0   5]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "yhat = neigh.predict(Xtest_transformed)\n",
    "\n",
    "#real labels\n",
    "ytest = testDataC['labels'].values.tolist()\n",
    "\n",
    "#accuracy\n",
    "print(accuracy_score(ytest,yhat))\n",
    "cm = confusion_matrix(ytest, yhat)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmPLqKO5Z8xc"
   },
   "source": [
    "## Using no HE images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QYP42Uk_Z-v1",
    "outputId": "39c88d57-73ae-43ce-95c8-93c7895fc7e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1300, 10)\n",
      "0.9061538461538462\n",
      "[[100   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0 100   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0  77   0   0   0   0   0   0  18   5   0   0]\n",
      " [  0   0   0 100   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 100   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0 100   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  99   0   0   1   0   0   0]\n",
      " [  0   0   0   0   0   0   0 100   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 100   0   0   0   0]\n",
      " [  0   0  19   0   0   0   0   0   0  69  12   0   0]\n",
      " [  0   0  17   0   0   0   0   0   0  14  57  12   0]\n",
      " [  0   0   3   0   0   0   0   0   0   7  14  76   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0 100]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "transformer = KernelPCA(n_components=10, kernel='precomputed')\n",
    "X_transformed = transformer.fit_transform(less_G)\n",
    "print(X_transformed.shape)\n",
    "\n",
    "#convert dataframe labels to list of targets\n",
    "ytrain = less_trainDataC['labels'].values.tolist()\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "neigh.fit(X_transformed, ytrain)\n",
    "\n",
    "yhat = neigh.predict(X_transformed)\n",
    "\n",
    "print(accuracy_score(ytrain,yhat))\n",
    "cm = confusion_matrix(ytrain, yhat)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gc-gP-fWbGUs"
   },
   "source": [
    "#find best value for sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_KDRHFrJbGGR",
    "outputId": "e45b8dfc-6c8a-4d37-d4b9-4284843f2fc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma is  0.1 \n",
      "\n",
      "(1300, 10)\n",
      "The accuracy with sigma =  0.1 is 0.9061538461538462 \n",
      "\n",
      "[[100   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0 100   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0  77   0   0   0   0   0   0  18   5   0   0]\n",
      " [  0   0   0 100   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 100   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0 100   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  99   0   0   1   0   0   0]\n",
      " [  0   0   0   0   0   0   0 100   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 100   0   0   0   0]\n",
      " [  0   0  19   0   0   0   0   0   0  69  12   0   0]\n",
      " [  0   0  17   0   0   0   0   0   0  14  57  12   0]\n",
      " [  0   0   3   0   0   0   0   0   0   7  14  76   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0 100]]\n",
      "********************\n",
      "sigma is  0.2 \n",
      "\n",
      "(1300, 10)\n",
      "The accuracy with sigma =  0.2 is 0.9907692307692307 \n",
      "\n",
      "[[100   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0 100   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0  99   0   0   0   0   0   0   0   1   0   0]\n",
      " [  0   0   0 100   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 100   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0 100   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0 100   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 100   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 100   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  97   3   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   8  92   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 100   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0 100]]\n",
      "********************\n",
      "sigma is  0.4 \n",
      "\n",
      "(1300, 10)\n",
      "The accuracy with sigma =  0.4 is 1.0 \n",
      "\n",
      "[[100   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0 100   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0 100   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 100   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 100   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0 100   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0 100   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 100   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 100   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 100   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 100   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 100   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0 100]]\n",
      "********************\n",
      "sigma is  0.8 \n",
      "\n",
      "(1300, 10)\n",
      "The accuracy with sigma =  0.8 is 1.0 \n",
      "\n",
      "[[100   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0 100   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0 100   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 100   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 100   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0 100   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0 100   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 100   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 100   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 100   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 100   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 100   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0 100]]\n",
      "********************\n",
      "sigma is  1 \n",
      "\n",
      "(1300, 10)\n",
      "The accuracy with sigma =  1 is 1.0 \n",
      "\n",
      "[[100   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0 100   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0 100   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 100   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 100   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0 100   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0 100   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 100   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 100   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 100   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 100   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 100   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0 100]]\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "sigmas = [0.1,0.2,0.4,0.8,1]\n",
    "\n",
    "for s in sigmas: \n",
    "  print('sigma is ',s,'\\n')\n",
    "\n",
    "  G_sigma = generate_gram_matrix(less_trainDataC,less_trainDataC,sigma=s)\n",
    "\n",
    "  np.save('/content/drive/MyDrive/ECE594N/hw3/'+'G_sigma'+str(s)+'.npy',G_sigma)\n",
    "  # less_G = np.load('/content/drive/MyDrive/ECE594N/hw3/'+'G_sigma'+str(s)+'.npy')\n",
    "\n",
    "\n",
    "  transformer = KernelPCA(n_components=10, kernel='precomputed')\n",
    "  X_transformed = transformer.fit_transform(G_sigma)\n",
    "  print(X_transformed.shape)\n",
    "\n",
    "  #convert dataframe labels to list of targets\n",
    "  ytrain = less_trainDataC['labels'].values.tolist()\n",
    "\n",
    "  neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "  neigh.fit(X_transformed, ytrain)\n",
    "\n",
    "  yhat = neigh.predict(X_transformed)\n",
    "\n",
    "  print('The accuracy with sigma = ',s, 'is',accuracy_score(ytrain,yhat),'\\n')\n",
    "  cm = confusion_matrix(ytrain, yhat)\n",
    "  print(cm)\n",
    "  print('********************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlUVMFt_lU4-"
   },
   "source": [
    "# using smallest good sigma value, sigma = 0.4, varying L, Riemannian Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bC_sco_LlWsd",
    "outputId": "63ec5a7a-aeef-4ebb-a314-5ef322d1f4fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "(1300, 10)\n",
      "1.0\n",
      "[[100   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0 100   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0 100   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 100   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 100   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0 100   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0 100   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 100   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 100   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 100   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 100   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 100   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0 100]]\n",
      "***********************\n",
      "using test data with l =  10\n",
      "Xtest_transformed is shape  (2600, 10)\n",
      "0.9857692307692307\n",
      "[[200   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0 199   0   0   0   1   0   0   0   0   0   0   0]\n",
      " [  0   0 197   0   0   0   0   0   0   3   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 199   1   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0 199   0   0   0   1   0   0   0]\n",
      " [  0   0   0   0   0   0 199   0   0   1   0   0   0]\n",
      " [  0   0   0   0   0   0   0 200   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 199   1   0   0   0]\n",
      " [  0   0   1   0   0   0   0   0   0 194   5   0   0]\n",
      " [  0   0   0   0   0   2   0   0   0  16 182   0   0]\n",
      " [  0   0   0   0   0   3   0   0   0   1   0 196   0]\n",
      " [  0   0   0   0   0   1   0   0   0   0   0   0 199]]\n",
      "*********************************************************************\n",
      "11\n",
      "(1300, 11)\n",
      "0.9992307692307693\n",
      "[[100   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0 100   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0 100   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 100   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 100   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0 100   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0 100   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 100   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 100   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 100   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   1  99   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 100   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0 100]]\n",
      "***********************\n",
      "using test data with l =  11\n",
      "Xtest_transformed is shape  (2600, 11)\n",
      "0.9857692307692307\n",
      "[[200   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0 199   0   0   0   1   0   0   0   0   0   0   0]\n",
      " [  0   0 198   0   0   0   0   0   0   2   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 199   1   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0 199   0   0   0   1   0   0   0]\n",
      " [  0   0   0   0   0   1 199   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 200   0   0   0   0   0]\n",
      " [  0   0   0   0   0   1   0   0 199   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 195   5   0   0]\n",
      " [  0   0   0   0   0   2   0   0   0  18 180   0   0]\n",
      " [  0   0   0   0   0   4   0   0   0   0   0 196   0]\n",
      " [  0   0   0   0   0   1   0   0   0   0   0   0 199]]\n",
      "*********************************************************************\n",
      "12\n",
      "(1300, 12)\n",
      "0.9984615384615385\n",
      "[[100   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0 100   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0 100   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 100   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 100   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0 100   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0 100   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 100   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 100   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 100   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   2  98   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 100   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0 100]]\n",
      "***********************\n",
      "using test data with l =  12\n",
      "Xtest_transformed is shape  (2600, 12)\n",
      "0.9842307692307692\n",
      "[[200   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0 199   0   0   0   1   0   0   0   0   0   0   0]\n",
      " [  0   0 198   0   0   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 199   0   0   0   0   1   0   0   0]\n",
      " [  0   0   0   0   0 200   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   1 199   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 200   0   0   0   0   0]\n",
      " [  0   0   0   0   0   1   0   0 199   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 194   6   0   0]\n",
      " [  0   0   0   0   0   2   0   0   0  22 176   0   0]\n",
      " [  0   0   0   0   0   4   0   0   0   0   0 196   0]\n",
      " [  0   0   0   0   0   1   0   0   0   0   0   0 199]]\n",
      "*********************************************************************\n",
      "15\n",
      "(1300, 15)\n",
      "1.0\n",
      "[[100   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0 100   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0 100   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 100   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 100   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0 100   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0 100   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 100   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 100   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 100   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 100   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 100   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0 100]]\n",
      "***********************\n",
      "using test data with l =  15\n",
      "Xtest_transformed is shape  (2600, 15)\n",
      "0.9942307692307693\n",
      "[[200   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0 199   0   0   0   1   0   0   0   0   0   0   0]\n",
      " [  0   0 198   0   0   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 199   1   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0 200   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   1 199   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 200   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 199   1   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 200   0   0   0]\n",
      " [  0   0   0   0   0   3   0   0   0   1 196   0   0]\n",
      " [  0   0   0   0   0   4   0   0   0   0   0 196   0]\n",
      " [  0   0   0   0   0   1   0   0   0   0   0   0 199]]\n",
      "*********************************************************************\n"
     ]
    }
   ],
   "source": [
    "ls = [10,11,12,15]\n",
    "for l in ls:\n",
    "  print(l)\n",
    "  s = .4\n",
    "  #load best sigma for gram(X,X)\n",
    "  G_fit = np.load('/content/drive/MyDrive/ECE594N/hw3/'+'G_sigma'+str(s)+'.npy')\n",
    "\n",
    "  #fit kernel pca\n",
    "  transformer = KernelPCA(n_components=l, kernel='precomputed')\n",
    "  X_transformed = transformer.fit_transform(G_fit)\n",
    "  print(X_transformed.shape)\n",
    "\n",
    "  #convert dataframe labels to list of targets\n",
    "  ytrain = less_trainDataC['labels'].values.tolist()\n",
    "\n",
    "  neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "  neigh.fit(X_transformed, ytrain)\n",
    "\n",
    "  yhat = neigh.predict(X_transformed)\n",
    "\n",
    "  print(accuracy_score(ytrain,yhat))\n",
    "  cm = confusion_matrix(ytrain, yhat)\n",
    "  print(cm)\n",
    "\n",
    "  #generate gram matrix for test data\n",
    "  print('***********************')\n",
    "  print('using test data with l = ',l)\n",
    "  # G_test = generate_gram_matrix(less_testDataC,less_trainDataC)\n",
    "  G_test = np.load('/content/drive/MyDrive/ECE594N/hw3/'+'G_test.npy')\n",
    "\n",
    "\n",
    "  Xtest_transformed = transformer.transform(G_test)\n",
    "  print('Xtest_transformed is shape ',Xtest_transformed.shape)\n",
    "\n",
    "  #convert dataframe labels to list of targets\n",
    "  ytest = less_testDataC['labels'].values.tolist()\n",
    "\n",
    "#   neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "#   neigh.fit(Xtest_transformed, ytest)\n",
    "\n",
    "  yhat = neigh.predict(Xtest_transformed)\n",
    "\n",
    "  print(accuracy_score(ytest, yhat))\n",
    "  cm = confusion_matrix(ytest, yhat)\n",
    "  print(cm)\n",
    "  print('*********************************************************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sN7iRxtFQo-k"
   },
   "source": [
    "#find best sigma for euclidean metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SDyIeOv5Qn1W"
   },
   "outputs": [],
   "source": [
    "sigmas = [0.1,0.2,0.4,0.8,1]\n",
    "\n",
    "for s in sigmas: \n",
    "  print('sigma is ',s,'\\n')\n",
    "\n",
    "  G_sigma = generate_gram_matrix(less_trainDataC,less_trainDataC,sigma=s)\n",
    "\n",
    "  np.save('/content/drive/MyDrive/ECE594N/hw3/'+'G_sigma'+str(s)+'.npy',G_sigma)\n",
    "  # less_G = np.load('/content/drive/MyDrive/ECE594N/hw3/'+'G_sigma'+str(s)+'.npy')\n",
    "\n",
    "\n",
    "  transformer = KernelPCA(n_components=10, kernel='precomputed')\n",
    "  X_transformed = transformer.fit_transform(G_sigma)\n",
    "  print(X_transformed.shape)\n",
    "\n",
    "  #convert dataframe labels to list of targets\n",
    "  ytrain = less_trainDataC['labels'].values.tolist()\n",
    "\n",
    "  neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "  neigh.fit(X_transformed, ytrain)\n",
    "\n",
    "  yhat = neigh.predict(X_transformed)\n",
    "\n",
    "  print('The accuracy with sigma = ',s, 'is',accuracy_score(ytrain,yhat),'\\n')\n",
    "  cm = confusion_matrix(ytrain, yhat)\n",
    "  print(cm)\n",
    "  print('********************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6T6hsm3Vt0wM"
   },
   "source": [
    "# using smallest good sigma value, sigma = 0.4, varying L, Euclidean Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qqNRTg27-oOo",
    "outputId": "85787ccb-af17-4042-b5bf-2a063d8f779e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3071.54866701,  -103.38451442,  -144.30252444, -3247.9420758 ,\n",
       "       -2498.17334023,  -103.38451442,  7278.38281417,    39.15689772,\n",
       "        1147.01569188,   157.22098374,  -144.30252444,    39.15689772,\n",
       "        5098.39688419,   221.38662067,  1057.89283258, -3247.9420758 ,\n",
       "        1147.01569188,   221.38662067, 11916.69841522,  1460.82044017,\n",
       "       -2498.17334023,   157.22098374,  1057.89283258,  1460.82044017,\n",
       "       10194.1671617 ])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "less_trainDataC['Covariance Descriptor'][0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "duny7ZWX_DjU",
    "outputId": "d4b77921-a19d-4603-d79d-2deadb5c64cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1300, 25)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain = []\n",
    "for i in range(len(less_trainDataC)):\n",
    "  xtrain.append(less_trainDataC['Covariance Descriptor'][i].flatten())\n",
    "\n",
    "xtrain = np.array(xtrain)\n",
    "xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-4eBM2XW_wlN",
    "outputId": "0126e4cd-0c4d-47a4-976d-737d115fa7bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2600, 25)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest = []\n",
    "for i in range(len(less_testDataC)):\n",
    "  xtest.append(less_testDataC['Covariance Descriptor'][i].flatten())\n",
    "\n",
    "xtest = np.array(xtest)\n",
    "xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXHAGYFfCKdH"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "pairwise_kernels(xtrain[0], xtrain[0], metric='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "id": "RwKPWNulC2yk"
   },
   "outputs": [],
   "source": [
    "def generate_gram_matrix_euc(X,Y):\n",
    "  '''\n",
    "  generates the gram matrix for using a euclidean kernel with kernel pca\n",
    "  X and Y are vectors\n",
    "  '''\n",
    "\n",
    "  #precomputing the gram matrix\n",
    "  G = []\n",
    "  for j in range(len(X)):\n",
    "    for i in range(len(Y)):\n",
    "      # G.append(X[i]@Y[i].T)\n",
    "      G.append(np.linalg.norm(X[j]-Y[i]))\n",
    "\n",
    "    # print(j)\n",
    "    \n",
    "  G = np.array(G)\n",
    "  G = np.reshape(G,(X.shape[0],Y.shape[0]))\n",
    "  return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "id": "eoVzXgl8DHcz"
   },
   "outputs": [],
   "source": [
    "Geuc_train = generate_gram_matrix_euc(xtrain,xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "id": "r9emuJ_-DdwA"
   },
   "outputs": [],
   "source": [
    "Geuc_test = generate_gram_matrix_euc(xtest,xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "id": "bNFLtxjtGONv"
   },
   "outputs": [],
   "source": [
    "def generate_gram_matrix_euc2(X,Y,sigma = 0.1):\n",
    "  '''\n",
    "  generates the gram matrix for using a custom kernel with kernel pca\n",
    "  X and Y are dataframes with 'Covariance Descriptor' columns\n",
    "  '''\n",
    "  #precomputing the gram matrix\n",
    "  G = []\n",
    "  for j in range(len(X['Covariance Descriptor'])):\n",
    "    for i in range(len(Y['Covariance Descriptor'])):\n",
    "      G.append(gs.exp( (-((np.linalg.norm(X['Covariance Descriptor'][j]-Y['Covariance Descriptor'][i]) ) **2)) / (2*(sigma**2))))\n",
    "\n",
    "    # print(j)\n",
    "    \n",
    "  G = np.array(G)\n",
    "  G = np.reshape(G,(X.shape[0],Y.shape[0]))\n",
    "  return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "id": "C1GGFN3UGy5i"
   },
   "outputs": [],
   "source": [
    "Geuc_train = generate_gram_matrix_euc2(less_trainDataC,less_trainDataC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "id": "P1hJ23vZGzp5"
   },
   "outputs": [],
   "source": [
    "Geuc_test = generate_gram_matrix_euc2(less_testDataC,less_trainDataC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-_5ts_Ryt2nn",
    "outputId": "c7aa09d7-00a5-49a2-a2b0-00b1127927cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "(1300, 10)\n",
      "0.07692307692307693\n",
      "[[  0   0   0 100   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 100   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 100   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 100   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 100   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 100   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 100   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 100   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 100   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 100   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 100   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 100   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 100   0   0   0   0   0   0   0   0   0]]\n",
      "***********************\n",
      "using test data with l =  10\n",
      "Xtest_transformed is shape  (2600, 10)\n",
      "0.07692307692307693\n",
      "[[  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]]\n",
      "*********************************************************************\n",
      "11\n",
      "(1300, 11)\n",
      "0.07692307692307693\n",
      "[[  0   0   0 100   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 100   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 100   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 100   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 100   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 100   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 100   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 100   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 100   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 100   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 100   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 100   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 100   0   0   0   0   0   0   0   0   0]]\n",
      "***********************\n",
      "using test data with l =  11\n",
      "Xtest_transformed is shape  (2600, 11)\n",
      "0.07692307692307693\n",
      "[[  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]]\n",
      "*********************************************************************\n",
      "12\n",
      "(1300, 12)\n",
      "0.5346153846153846\n",
      "[[86  0  0 13  0  0  0  0  0  1  0  0  0]\n",
      " [ 0 27  0  0  0  0  0 10  0  0  0  0 63]\n",
      " [ 0  0 75  0  0 20  5  0  0  0  0  0  0]\n",
      " [10  1  0 89  0  0  0  0  0  0  0  0  0]\n",
      " [70  0  0  2 15  0  0  0  0 12  0  0  1]\n",
      " [ 0  0  2  0  0 86  0  0 12  0  0  0  0]\n",
      " [ 0  0 12  0  0  0 88  0  0  0  0  0  0]\n",
      " [ 0 16  0  0  0  0  0 39  0  0  0  0 45]\n",
      " [ 0  7  0  0  0  4  0 19 63  0  0  0  7]\n",
      " [86  0  0  0  0  0  0  0  0 13  1  0  0]\n",
      " [ 0  7  0  0  0  0  0  0  0  0 38  0 55]\n",
      " [ 0 19  0  0  0  0  0 16  0  0 10  0 55]\n",
      " [ 8  3  0  0  0  0  0  0  0 12  1  0 76]]\n",
      "***********************\n",
      "using test data with l =  12\n",
      "Xtest_transformed is shape  (2600, 12)\n",
      "0.11538461538461539\n",
      "[[  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0 100   0 100   0   0   0   0   0   0   0   0   0]\n",
      " [  0 200   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0 200   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0 198   0   2   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0 198   0   2   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0 200   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0 200   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]]\n",
      "*********************************************************************\n",
      "15\n",
      "(1300, 15)\n",
      "0.3553846153846154\n",
      "[[73  1  1  2  2  2  2  1  1  7  2  3  3]\n",
      " [16 55  2  1  2  3  2  4  5  5  2  2  1]\n",
      " [13 15 48  1  6  3  3  5  0  1  1  2  2]\n",
      " [13 15  9 44  2  4  3  0  2  2  1  2  3]\n",
      " [ 8 18 17  9 28  2  3  6  0  1  4  1  3]\n",
      " [14  8 11  4  9 28  1  3  7  2  4  6  3]\n",
      " [ 6 17 12  8  8  3 24  6  5  5  2  4  0]\n",
      " [12 12 12  5  6  8  6 26  4  3  5  0  1]\n",
      " [17 17  6  8  4  4  7  2 29  0  3  2  1]\n",
      " [19 12 11  7  6  2  2  3  3 27  1  3  4]\n",
      " [17  8  9  5  4  8  2  4  6  1 30  3  3]\n",
      " [14 17  6  6  3  2  3  4  3  4  5 28  5]\n",
      " [20 15  5  8  8  5  2  2  1  5  2  5 22]]\n",
      "***********************\n",
      "using test data with l =  15\n",
      "Xtest_transformed is shape  (2600, 15)\n",
      "0.07692307692307693\n",
      "[[  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 200   0   0   0   0   0   0   0   0   0]]\n",
      "*********************************************************************\n"
     ]
    }
   ],
   "source": [
    "ls = [10,11,12,15]\n",
    "from sklearn.decomposition import PCA\n",
    "for l in ls:\n",
    "  print(l)\n",
    "  s = 0.4\n",
    "  #load best sigma for gram(X,X)\n",
    "  G_fit = np.load('/content/drive/MyDrive/ECE594N/hw3/'+'G_sigma'+str(s)+'.npy')\n",
    "\n",
    "  #fit kernel pca\n",
    "  transformer = KernelPCA(n_components=l,kernel='precomputed')\n",
    "  X_transformed = transformer.fit_transform(Geuc_train)\n",
    "  print(X_transformed.shape)\n",
    "\n",
    "  #convert dataframe labels to list of targets\n",
    "  ytrain = less_trainDataC['labels'].values.tolist()\n",
    "\n",
    "  neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "  neigh.fit(X_transformed, ytrain)\n",
    "\n",
    "  yhat = neigh.predict(X_transformed)\n",
    "\n",
    "  print(accuracy_score(ytrain,yhat))\n",
    "  cm = confusion_matrix(ytrain, yhat)\n",
    "  print(cm)\n",
    "\n",
    "  #generate gram matrix for test data\n",
    "  print('***********************')\n",
    "  print('using test data with l = ',l)\n",
    "  # G_test = generate_gram_matrix(less_testDataC,less_trainDataC)\n",
    "  # G_test = np.load('/content/drive/MyDrive/ECE594N/hw3/'+'G_test.npy')\n",
    "\n",
    "\n",
    "  Xtest_transformed = transformer.transform(Geuc_test)\n",
    "\n",
    "  print('Xtest_transformed is shape ',Xtest_transformed.shape)\n",
    "\n",
    "  #convert dataframe labels to list of targets\n",
    "  ytest = less_testDataC['labels'].values.tolist()\n",
    "\n",
    "#   neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "#   neigh.fit(Xtest_transformed, ytest)\n",
    "\n",
    "  yhat = neigh.predict(Xtest_transformed)\n",
    "\n",
    "  print(accuracy_score(ytest, yhat))\n",
    "  cm = confusion_matrix(ytest, yhat)\n",
    "  print(cm)\n",
    "  print('*********************************************************************')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ECE594N HW3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
